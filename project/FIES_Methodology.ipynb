{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a06fbb",
   "metadata": {},
   "source": [
    "# Philippine Family Income and Expenditure (FIES) Analysis\n",
    "## Income Classification, Household Clustering, Anomaly Detection & Dimensionality Reduction\n",
    "\n",
    "**Course:** AI 221 Classical Machine Learning  \n",
    "**Data Source:** Kaggle - Family Income and Expenditure Survey (Philippine Statistics Authority)  \n",
    "**Dataset Size:** 40,000+ households × 60 variables\n",
    "\n",
    "### Project Overview\n",
    "This project addresses the critical problem of **socio-economic classification in the Philippines** using machine learning methods covered in AI 221. We combine supervised and unsupervised learning to:\n",
    "\n",
    "1. **Supervised Learning:** Predict household income levels from expenditure patterns\n",
    "2. **Unsupervised Learning:** Discover natural household segments and detect anomalies\n",
    "3. **Visualization:** Reduce dimensionality to understand the household landscape\n",
    "\n",
    "---\n",
    "\n",
    "## Project Objectives\n",
    "\n",
    "- **Supervised:** Train classification models to predict income bracket based on household expenditures\n",
    "- **Unsupervised:** Identify distinct household clusters and detect unusual spending patterns\n",
    "- **Analysis:** Extract actionable insights about Filipino household economics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de20c68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Data Loading and Exploratory Analysis\n",
    "\n",
    "### Methodology\n",
    "\n",
    "**Step 1.1: Data Loading**\n",
    "- Load the FIES dataset from CSV file\n",
    "- Check dataset shape, data types, and basic information\n",
    "- Identify target variables and feature variables\n",
    "\n",
    "**Step 1.2: Missing Values Analysis**\n",
    "- Quantify missing values per column\n",
    "- Decide on imputation strategy (mean, median, forward-fill, or removal)\n",
    "- Document handling decisions\n",
    "\n",
    "**Step 1.3: Exploratory Data Analysis (EDA)**\n",
    "- Summary statistics (mean, std, min, max, quartiles)\n",
    "- Distribution analysis of income and major expenditure categories\n",
    "- Identify skewed distributions and outliers\n",
    "- Correlation analysis between features\n",
    "- Geographic and demographic breakdowns\n",
    "\n",
    "**Step 1.4: Feature Understanding**\n",
    "- Categorize variables: Demographics, Income sources, Expenditure categories\n",
    "- Create income brackets/levels (e.g., Quintiles: Q1-Q5, or Low/Middle/High)\n",
    "- Document all 60 variables and their roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e705d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: LOAD DATA\n",
    "# Replace 'filepath' with actual Kaggle dataset path\n",
    "# Download from: https://www.kaggle.com/datasets/grosvenpaul/family-income-and-expenditure\n",
    "\n",
    "# df = pd.read_csv('path_to_dataset/Family Income and Expenditure.csv')\n",
    "# For demonstration, we'll create sample structure\n",
    "\n",
    "# Display basic information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET LOADING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n# TODO: Load your dataset with:\")\n",
    "print(\"df = pd.read_csv('path_to/Family Income and Expenditure.csv')\")\n",
    "print(\"\\n# After loading, run these exploratory commands:\"\n",
    "print(\"print(df.shape)\")\n",
    "print(\"print(df.info())\")\n",
    "print(\"print(df.head())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a296e2ac",
   "metadata": {},
   "source": [
    "### Data Exploration Code\n",
    "After loading the dataset, perform these analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: MISSING VALUES ANALYSIS\n",
    "# Check missing values\n",
    "# missing = df.isnull().sum()\n",
    "# missing_percent = (missing / len(df)) * 100\n",
    "# missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_percent})\n",
    "# print(missing_df[missing_df['Missing_Count'] > 0].sort_values('Percentage', ascending=False))\n",
    "\n",
    "print(\"Check for missing values in your dataset\")\n",
    "print(\"Handle missing values: drop or impute based on percentage missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6584a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: SUMMARY STATISTICS\n",
    "# print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "# print(df.describe())\n",
    "# print(\"\\nData Types:\")\n",
    "# print(df.dtypes)\n",
    "\n",
    "print(\"Summary Statistics Template:\")\n",
    "print(\"- Review mean, std, min, max for all features\")\n",
    "print(\"- Check for unrealistic values (negative income, etc.)\")\n",
    "print(\"- Identify potential data entry errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc8914",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Income Level Classification (SUPERVISED LEARNING)\n",
    "\n",
    "### Methodology Overview\n",
    "\n",
    "**Goal:** Predict household income bracket from expenditure patterns using supervised classification.\n",
    "\n",
    "**Approach:**\n",
    "1. Create income target variable (income brackets/quintiles)\n",
    "2. Select relevant features (expenditure categories, demographics)\n",
    "3. Preprocess: Scale features, handle missing values\n",
    "4. Split data: 80% train, 20% test\n",
    "5. Train multiple classifiers\n",
    "6. Evaluate and compare model performance\n",
    "\n",
    "### Detailed Steps\n",
    "\n",
    "#### Step 2.1: Create Target Variable - Income Brackets\n",
    "\n",
    "**Method:** Create income quintiles (Q1-Q5) or categories (Low/Middle/High)\n",
    "\n",
    "```python\n",
    "# Option 1: Quintiles (5 classes)\n",
    "df['income_quintile'] = pd.qcut(df['Total Income'], q=5, \n",
    "                                labels=['Q1_Lowest', 'Q2_Low', 'Q3_Middle', 'Q4_High', 'Q5_Highest'])\n",
    "\n",
    "# Option 2: Simple categories (3 classes) - Better for classification\n",
    "income_threshold_low = df['Total Income'].quantile(0.33)\n",
    "income_threshold_high = df['Total Income'].quantile(0.67)\n",
    "\n",
    "df['income_level'] = pd.cut(df['Total Income'], \n",
    "                            bins=[-np.inf, income_threshold_low, income_threshold_high, np.inf],\n",
    "                            labels=['Low_Income', 'Middle_Income', 'High_Income'])\n",
    "```\n",
    "\n",
    "#### Step 2.2: Feature Selection\n",
    "\n",
    "**Selected Features:**\n",
    "- **Expenditure Categories:** Total food, housing, utilities, healthcare, education, transportation\n",
    "- **Demographics:** Family size, number of workers, region (urban/rural)\n",
    "- **Income Sources:** Main income, secondary income, remittances\n",
    "- **Ratios:** Food-to-income ratio, savings rate (if available)\n",
    "\n",
    "**Excluded:** Target variable itself, identification columns\n",
    "\n",
    "```python\n",
    "# Define feature columns\n",
    "expenditure_cols = ['Total Food', 'Housing', 'Utilities', 'Healthcare', \n",
    "                    'Education', 'Transportation', 'Clothing', 'Personal Care']\n",
    "demographic_cols = ['Family Size', 'Number of Workers', 'Region']\n",
    "income_cols = ['Wage Income', 'Self-Employment Income', 'Other Income']\n",
    "\n",
    "feature_cols = expenditure_cols + demographic_cols + income_cols\n",
    "X = df[feature_cols]\n",
    "y = df['income_level']  # or 'income_quintile'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f2cf0",
   "metadata": {},
   "source": [
    "#### Step 2.3: Data Preprocessing\n",
    "\n",
    "**Normalization/Standardization:**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "```\n",
    "\n",
    "**Why standardize?**\n",
    "- Different features have different scales (income in thousands, family size in single digits)\n",
    "- Required for SVM, distance-based methods (Week 3-4)\n",
    "- Ensures fair feature contribution\n",
    "\n",
    "**Handle Categorical Variables:**\n",
    "```python\n",
    "# Encode categorical features\n",
    "le = LabelEncoder()\n",
    "X_scaled['Region'] = le.fit_transform(X_scaled['Region'])\n",
    "```\n",
    "\n",
    "#### Step 2.4: Train-Test Split\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, \n",
    "                                                      test_size=0.2, \n",
    "                                                      random_state=42,\n",
    "                                                      stratify=y)  # Maintain class distribution\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"\\nClass distribution in training set:\\n{y_train.value_counts(normalize=True)}\")\n",
    "```\n",
    "\n",
    "#### Step 2.5: Classification Models\n",
    "\n",
    "**Model 1: Logistic Regression (Week 3)**\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Why:** Baseline model, interpretable, fast training, probability outputs\n",
    "\n",
    "---\n",
    "\n",
    "**Model 2: Support Vector Machine (Week 4)**\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Why:** Handles high-dimensional data, good for multi-class problems, effective in practice\n",
    "\n",
    "**Hyperparameters to tune:**\n",
    "- `kernel`: 'linear', 'rbf', 'poly' (Week 5)\n",
    "- `C`: Regularization strength (larger C = less regularization)\n",
    "- `gamma`: Kernel coefficient (only for rbf/poly)\n",
    "\n",
    "---\n",
    "\n",
    "**Model 3: Gradient Boosting (Week 8 - Ensemble Learning)**\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, \n",
    "                                      learning_rate=0.1,\n",
    "                                      max_depth=5,\n",
    "                                      random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Why:** Powerful ensemble method, handles feature interactions, provides feature importance\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `n_estimators`: Number of boosting stages (100-300)\n",
    "- `learning_rate`: Shrinkage/step size (0.01-0.1)\n",
    "- `max_depth`: Tree depth (3-8)\n",
    "\n",
    "---\n",
    "\n",
    "**Model 4: Random Forest (Week 8 - Ensemble Learning)**\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100,\n",
    "                                  max_depth=10,\n",
    "                                  min_samples_split=5,\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Why:** Robust, handles non-linear relationships, feature importance, parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26dd0c5",
   "metadata": {},
   "source": [
    "#### Step 2.6: Model Evaluation Metrics\n",
    "\n",
    "**Metrics to Report:**\n",
    "\n",
    "1. **Accuracy:** Overall correctness\n",
    "   $$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "2. **Precision:** Of predicted positives, how many are correct\n",
    "   $$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "3. **Recall (Sensitivity):** Of actual positives, how many did we find\n",
    "   $$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "4. **F1-Score:** Harmonic mean of precision and recall\n",
    "   $$F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "5. **Confusion Matrix:** Shows TP, FP, TN, FN for each class\n",
    "\n",
    "**Evaluation Code:**\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# For each model\n",
    "print(f\"=== Model Performance ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "```\n",
    "\n",
    "#### Step 2.7: Model Comparison\n",
    "\n",
    "Create a comparison table of all models:\n",
    "```python\n",
    "results_dict = {\n",
    "    'Logistic Regression': {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "        'Precision': precision_score(y_test, y_pred_lr, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_pred_lr, average='weighted'),\n",
    "        'F1-Score': f1_score(y_test, y_pred_lr, average='weighted')\n",
    "    },\n",
    "    'SVM': {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_svm),\n",
    "        'Precision': precision_score(y_test, y_pred_svm, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_pred_svm, average='weighted'),\n",
    "        'F1-Score': f1_score(y_test, y_pred_svm, average='weighted')\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_gb),\n",
    "        'Precision': precision_score(y_test, y_pred_gb, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_pred_gb, average='weighted'),\n",
    "        'F1-Score': f1_score(y_test, y_pred_gb, average='weighted')\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "        'Precision': precision_score(y_test, y_pred_rf, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_pred_rf, average='weighted'),\n",
    "        'F1-Score': f1_score(y_test, y_pred_rf, average='weighted')\n",
    "    }\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "#### Step 2.8: Feature Importance (for tree-based models)\n",
    "\n",
    "```python\n",
    "# For Gradient Boosting and Random Forest\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance_gb.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance_gb.head(10), x='Importance', y='Feature')\n",
    "plt.title('Top 10 Feature Importance - Gradient Boosting')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff9db7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Household Clustering and Segmentation (UNSUPERVISED LEARNING)\n",
    "\n",
    "### Methodology Overview\n",
    "\n",
    "**Goal:** Discover natural groupings of households with similar expenditure patterns without using income labels.\n",
    "\n",
    "**Approach:**\n",
    "1. Select relevant expenditure features\n",
    "2. Standardize all features (critical for clustering)\n",
    "3. Determine optimal number of clusters\n",
    "4. Apply multiple clustering algorithms\n",
    "5. Profile each cluster\n",
    "6. Interpret household segments\n",
    "\n",
    "### Detailed Steps\n",
    "\n",
    "#### Step 3.1: Feature Selection for Clustering\n",
    "\n",
    "**Features:** Expenditure categories normalized by total income\n",
    "\n",
    "```python\n",
    "# Prepare clustering data\n",
    "expenditure_features = ['Total Food', 'Housing', 'Utilities', 'Healthcare', \n",
    "                       'Education', 'Transportation', 'Clothing', 'Personal Care']\n",
    "\n",
    "# Create expenditure ratios (as percentage of total income)\n",
    "X_cluster = df[expenditure_features].copy()\n",
    "for col in expenditure_features:\n",
    "    X_cluster[col] = X_cluster[col] / df['Total Income']\n",
    "\n",
    "# Add demographic features\n",
    "X_cluster['Family Size'] = df['Family Size']\n",
    "X_cluster['Number of Workers'] = df['Number of Workers']\n",
    "\n",
    "# Standardize features\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "X_cluster_scaled = pd.DataFrame(X_cluster_scaled, columns=X_cluster.columns)\n",
    "```\n",
    "\n",
    "**Why these features?**\n",
    "- Expenditure ratios are comparable across income levels\n",
    "- Family size influences spending patterns\n",
    "- Number of workers indicates earning capacity\n",
    "\n",
    "#### Step 3.2: Determine Optimal Number of Clusters\n",
    "\n",
    "**Method 1: Elbow Method**\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Test different numbers of clusters\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(X_cluster_scaled)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_cluster_scaled, kmeans_temp.labels_))\n",
    "\n",
    "# Plot elbow curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(K_range, inertias, 'bo-')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia (Within-cluster SS)')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(K_range, silhouette_scores, 'ro-')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis')\n",
    "axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Silhouette Scores:\")\n",
    "for k, score in zip(K_range, silhouette_scores):\n",
    "    print(f\"k={k}: {score:.4f}\")\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- **Elbow point:** Where inertia decreases slower (typical k=3-5)\n",
    "- **Silhouette Score:** Higher is better (range -1 to 1). Choose k with highest score.\n",
    "\n",
    "**Method 2: Silhouette Coefficient**\n",
    "- Measures how similar an object is to its own cluster vs other clusters\n",
    "- Range: -1 (bad) to 1 (good)\n",
    "- Values > 0.5 indicate well-separated clusters\n",
    "\n",
    "#### Step 3.3: K-Means Clustering (Week 11)\n",
    "\n",
    "**Algorithm Overview:**\n",
    "1. Randomly initialize k centroids\n",
    "2. Assign each point to nearest centroid\n",
    "3. Update centroids as mean of assigned points\n",
    "4. Repeat until convergence\n",
    "\n",
    "```python\n",
    "# Apply K-Means with optimal k (e.g., k=4)\n",
    "optimal_k = 4  # Based on elbow/silhouette analysis\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10, max_iter=300)\n",
    "df['Cluster_KMeans'] = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "print(f\"K-Means Clustering (k={optimal_k})\")\n",
    "print(f\"Cluster distribution:\\n{df['Cluster_KMeans'].value_counts().sort_index()}\")\n",
    "print(f\"Within-cluster sum of squares: {kmeans.inertia_:.2f}\")\n",
    "```\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `n_clusters`: Number of clusters (from elbow method)\n",
    "- `random_state`: For reproducibility\n",
    "- `n_init`: Number of times to run (default=10)\n",
    "- `max_iter`: Maximum iterations (default=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080a1d2",
   "metadata": {},
   "source": [
    "#### Step 3.4: Hierarchical Clustering (Week 11)\n",
    "\n",
    "**Algorithm:** Agglomerative (bottom-up) approach\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "# Compute linkage matrix\n",
    "linkage_matrix = sch.linkage(X_cluster_scaled, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(14, 7))\n",
    "dendrogram = sch.dendrogram(linkage_matrix, \n",
    "                             leaf_font_size=10,\n",
    "                             no_labels=False)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward Linkage)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.axhline(y=50, c='r', linestyle='--', label='Cut-off line')  # Adjust height for k clusters\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Apply Hierarchical Clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "df['Cluster_Hierarchical'] = hierarchical.fit_predict(X_cluster_scaled)\n",
    "\n",
    "print(f\"Hierarchical Clustering (k={optimal_k})\")\n",
    "print(f\"Cluster distribution:\\n{df['Cluster_Hierarchical'].value_counts().sort_index()}\")\n",
    "```\n",
    "\n",
    "**Linkage Methods:**\n",
    "- `ward`: Minimizes variance (recommended)\n",
    "- `complete`: Maximum distance between clusters\n",
    "- `average`: Average distance between clusters\n",
    "- `single`: Minimum distance (can create chain effect)\n",
    "\n",
    "#### Step 3.5: DBSCAN Clustering (Week 11 - Density-based)\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Find optimal eps using k-distance graph\n",
    "neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors_fit = neighbors.fit(X_cluster_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(X_cluster_scaled)\n",
    "distances = np.sort(distances[:, -1], axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(distances)\n",
    "plt.ylabel('5-NN Distance')\n",
    "plt.xlabel('Data Points sorted by distance')\n",
    "plt.title('K-distance Graph for eps Selection')\n",
    "plt.axhline(y=2.5, c='r', linestyle='--', label='Possible eps')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Apply DBSCAN with eps from k-distance graph\n",
    "dbscan = DBSCAN(eps=2.5, min_samples=5)\n",
    "df['Cluster_DBSCAN'] = dbscan.fit_predict(X_cluster_scaled)\n",
    "\n",
    "n_clusters_db = len(set(df['Cluster_DBSCAN'])) - (1 if -1 in df['Cluster_DBSCAN'] else 0)\n",
    "n_noise = list(df['Cluster_DBSCAN']).count(-1)\n",
    "\n",
    "print(f\"DBSCAN Clustering Results:\")\n",
    "print(f\"Number of clusters: {n_clusters_db}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "print(f\"Cluster distribution:\\n{df['Cluster_DBSCAN'].value_counts().sort_index()}\")\n",
    "```\n",
    "\n",
    "**DBSCAN Parameters:**\n",
    "- `eps`: Maximum distance between points (from k-distance graph)\n",
    "- `min_samples`: Minimum points in neighborhood to form a cluster\n",
    "- Returns -1 for noise points\n",
    "\n",
    "#### Step 3.6: Cluster Profiling and Interpretation\n",
    "\n",
    "```python\n",
    "# Profile K-Means clusters\n",
    "cluster_profiles = df.groupby('Cluster_KMeans')[expenditure_features].mean()\n",
    "print(\"\\n=== Cluster Profiles (Average Expenditure Ratios) ===\")\n",
    "print(cluster_profiles)\n",
    "\n",
    "# Create interpretation\n",
    "cluster_names = {\n",
    "    0: 'Budget-Conscious Families',\n",
    "    1: 'Balanced Spenders',\n",
    "    2: 'Health & Education Focused',\n",
    "    3: 'High-Income Spenders'\n",
    "    # Adjust based on actual profiles\n",
    "}\n",
    "\n",
    "df['Cluster_Name'] = df['Cluster_KMeans'].map(cluster_names)\n",
    "\n",
    "# Visualize cluster characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for i, col in enumerate(expenditure_features[:4]):\n",
    "    ax = axes[i//2, i%2]\n",
    "    df.boxplot(column=col, by='Cluster_Name', ax=ax)\n",
    "    ax.set_title(f'{col} by Cluster')\n",
    "    ax.set_xlabel('Cluster')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics by cluster\n",
    "for cluster in sorted(df['Cluster_KMeans'].unique()):\n",
    "    cluster_data = df[df['Cluster_KMeans'] == cluster]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Cluster {cluster}: {cluster_names[cluster]}\")\n",
    "    print(f\"Size: {len(cluster_data)} households\")\n",
    "    print(f\"Average Total Income: ₱{cluster_data['Total Income'].mean():.2f}\")\n",
    "    print(f\"Average Family Size: {cluster_data['Family Size'].mean():.2f}\")\n",
    "    print(f\"Average Workers per Household: {cluster_data['Number of Workers'].mean():.2f}\")\n",
    "    print(f\"Top Spending Category: {cluster_data[expenditure_features].mean().idxmax()}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4161fe8f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Anomaly Detection (UNSUPERVISED LEARNING)\n",
    "\n",
    "### Methodology Overview\n",
    "\n",
    "**Goal:** Identify unusual household spending patterns that deviate significantly from normal behavior.\n",
    "\n",
    "**Why important:**\n",
    "- Detect data entry errors\n",
    "- Identify extremely high/low spenders\n",
    "- Find unusual budget allocations\n",
    "- Potential fraud or special circumstances\n",
    "\n",
    "**Approach:** Apply multiple anomaly detection methods and compare results\n",
    "\n",
    "### Detailed Steps\n",
    "\n",
    "#### Step 4.1: Isolation Forest (Week 11 - Density-based Anomaly Detection)\n",
    "\n",
    "**Algorithm Concept:**\n",
    "- Isolates anomalies by randomly selecting features and thresholds\n",
    "- Anomalies are isolated faster than normal points\n",
    "- Anomalies have shorter path lengths\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Apply Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05,  # Expect 5% anomalies\n",
    "                             random_state=42,\n",
    "                             n_estimators=100)\n",
    "anomaly_if = iso_forest.fit_predict(X_cluster_scaled)\n",
    "anomaly_score_if = iso_forest.score_samples(X_cluster_scaled)\n",
    "\n",
    "df['Anomaly_IsolationForest'] = anomaly_if\n",
    "df['Anomaly_Score_IF'] = anomaly_score_if\n",
    "\n",
    "n_anomalies_if = sum(anomaly_if == -1)\n",
    "print(f\"Isolation Forest:\")\n",
    "print(f\"Number of anomalies detected: {n_anomalies_if}\")\n",
    "print(f\"Percentage: {(n_anomalies_if/len(df))*100:.2f}%\")\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `contamination`: Expected proportion of anomalies (default=0.1)\n",
    "- `random_state`: For reproducibility\n",
    "- `n_estimators`: Number of isolation trees (default=100)\n",
    "\n",
    "**Output:** -1 for anomalies, 1 for normal points\n",
    "\n",
    "#### Step 4.2: Local Outlier Factor (LOF) (Week 11 - Density-based)\n",
    "\n",
    "**Algorithm Concept:**\n",
    "- Compares local density of a point to its neighbors\n",
    "- Points with much lower density than neighbors are outliers\n",
    "- Good for varying density regions\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "anomaly_lof = lof.fit_predict(X_cluster_scaled)\n",
    "anomaly_score_lof = lof.negative_outlier_factor_\n",
    "\n",
    "df['Anomaly_LOF'] = anomaly_lof\n",
    "df['Anomaly_Score_LOF'] = anomaly_score_lof\n",
    "\n",
    "n_anomalies_lof = sum(anomaly_lof == -1)\n",
    "print(f\"\\nLocal Outlier Factor (LOF):\")\n",
    "print(f\"Number of anomalies detected: {n_anomalies_lof}\")\n",
    "print(f\"Percentage: {(n_anomalies_lof/len(df))*100:.2f}%\")\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `n_neighbors`: Number of neighbors to consider (default=20)\n",
    "- `contamination`: Expected proportion of anomalies\n",
    "\n",
    "#### Step 4.3: Mahalanobis Distance / Statistical Method\n",
    "\n",
    "```python\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate mean and covariance\n",
    "mean = np.mean(X_cluster_scaled, axis=0)\n",
    "cov = np.cov(X_cluster_scaled.T)\n",
    "\n",
    "# Calculate Mahalanobis distance for each point\n",
    "mahal_dist = []\n",
    "for i in range(len(X_cluster_scaled)):\n",
    "    diff = X_cluster_scaled[i] - mean\n",
    "    mahal_dist.append(np.sqrt(diff.T @ np.linalg.inv(cov) @ diff))\n",
    "\n",
    "df['Mahal_Distance'] = mahal_dist\n",
    "\n",
    "# Threshold: Chi-square distribution with df = number of features\n",
    "threshold = stats.chi2.ppf(0.95, df=X_cluster_scaled.shape[1])\n",
    "anomaly_mahal = [1 if d > threshold else -1 for d in mahal_dist]\n",
    "df['Anomaly_Mahal'] = anomaly_mahal\n",
    "\n",
    "n_anomalies_mahal = sum(np.array(anomaly_mahal) == -1)\n",
    "print(f\"\\nMahalanobis Distance:\")\n",
    "print(f\"Number of anomalies detected: {n_anomalies_mahal}\")\n",
    "print(f\"Percentage: {(n_anomalies_mahal/len(df))*100:.2f}%\")\n",
    "```\n",
    "\n",
    "#### Step 4.4: Z-Score Based Anomaly Detection\n",
    "\n",
    "```python\n",
    "# Calculate Z-scores\n",
    "z_scores = np.abs(stats.zscore(X_cluster_scaled))\n",
    "\n",
    "# Flag as anomaly if any feature has |z-score| > 3\n",
    "anomaly_zscore = [-1 if (z_scores[i] > 3).any() else 1 for i in range(len(z_scores))]\n",
    "df['Anomaly_ZScore'] = anomaly_zscore\n",
    "\n",
    "n_anomalies_zscore = sum(np.array(anomaly_zscore) == -1)\n",
    "print(f\"\\nZ-Score Method:\")\n",
    "print(f\"Number of anomalies detected: {n_anomalies_zscore}\")\n",
    "print(f\"Percentage: {(n_anomalies_zscore/len(df))*100:.2f}%\")\n",
    "```\n",
    "\n",
    "**Threshold Interpretation:**\n",
    "- $|z| > 2$: Moderately unusual (95% confidence)\n",
    "- $|z| > 3$: Very unusual (99.7% confidence)\n",
    "\n",
    "#### Step 4.5: Consensus Anomaly Detection\n",
    "\n",
    "```python\n",
    "# Count how many methods flagged each point as anomaly\n",
    "anomaly_count = (\n",
    "    (df['Anomaly_IsolationForest'] == -1).astype(int) +\n",
    "    (df['Anomaly_LOF'] == -1).astype(int) +\n",
    "    (df['Anomaly_Mahal'] == -1).astype(int) +\n",
    "    (df['Anomaly_ZScore'] == -1).astype(int)\n",
    ")\n",
    "\n",
    "df['Anomaly_Count'] = anomaly_count\n",
    "df['Is_Anomaly_Consensus'] = (anomaly_count >= 2).astype(int)  # Flagged by 2+ methods\n",
    "\n",
    "consensus_anomalies = df[df['Is_Anomaly_Consensus'] == 1]\n",
    "print(f\"\\nConsensus Anomaly Detection (2+ methods agree):\")\n",
    "print(f\"Number of anomalies: {len(consensus_anomalies)}\")\n",
    "print(f\"Percentage: {(len(consensus_anomalies)/len(df))*100:.2f}%\")\n",
    "\n",
    "# Method agreement\n",
    "print(\"\\nMethod Agreement Summary:\")\n",
    "print(f\"Flagged by 1 method: {sum(anomaly_count == 1)}\")\n",
    "print(f\"Flagged by 2 methods: {sum(anomaly_count == 2)}\")\n",
    "print(f\"Flagged by 3 methods: {sum(anomaly_count == 3)}\")\n",
    "print(f\"Flagged by all 4 methods: {sum(anomaly_count == 4)}\")\n",
    "```\n",
    "\n",
    "#### Step 4.6: Anomaly Visualization\n",
    "\n",
    "```python\n",
    "# Visualize anomalies using scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Isolation Forest\n",
    "scatter1 = axes[0, 0].scatter(X_cluster_scaled.iloc[:, 0], \n",
    "                              X_cluster_scaled.iloc[:, 1],\n",
    "                              c=df['Anomaly_IsolationForest'],\n",
    "                              cmap='RdYlGn', s=50, alpha=0.6)\n",
    "axes[0, 0].set_title('Isolation Forest Anomalies')\n",
    "axes[0, 0].set_xlabel('Feature 1')\n",
    "axes[0, 0].set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter1, ax=axes[0, 0])\n",
    "\n",
    "# Plot 2: LOF\n",
    "scatter2 = axes[0, 1].scatter(X_cluster_scaled.iloc[:, 0],\n",
    "                              X_cluster_scaled.iloc[:, 1],\n",
    "                              c=df['Anomaly_LOF'],\n",
    "                              cmap='RdYlGn', s=50, alpha=0.6)\n",
    "axes[0, 1].set_title('Local Outlier Factor Anomalies')\n",
    "axes[0, 1].set_xlabel('Feature 1')\n",
    "axes[0, 1].set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter2, ax=axes[0, 1])\n",
    "\n",
    "# Plot 3: Mahalanobis Distance\n",
    "scatter3 = axes[1, 0].scatter(X_cluster_scaled.iloc[:, 0],\n",
    "                              X_cluster_scaled.iloc[:, 1],\n",
    "                              c=df['Mahal_Distance'],\n",
    "                              cmap='viridis', s=50, alpha=0.6)\n",
    "axes[1, 0].set_title('Mahalanobis Distance')\n",
    "axes[1, 0].set_xlabel('Feature 1')\n",
    "axes[1, 0].set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter3, ax=axes[1, 0])\n",
    "\n",
    "# Plot 4: Consensus\n",
    "scatter4 = axes[1, 1].scatter(X_cluster_scaled.iloc[:, 0],\n",
    "                              X_cluster_scaled.iloc[:, 1],\n",
    "                              c=df['Anomaly_Count'],\n",
    "                              cmap='YlOrRd', s=50, alpha=0.6)\n",
    "axes[1, 1].set_title('Consensus Anomaly Score (# Methods)')\n",
    "axes[1, 1].set_xlabel('Feature 1')\n",
    "axes[1, 1].set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter4, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Step 4.7: Analyze Anomalous Households\n",
    "\n",
    "```python\n",
    "# Examine consensus anomalies\n",
    "print(\"\\n=== TOP 10 ANOMALOUS HOUSEHOLDS ===\")\n",
    "top_anomalies = df.nlargest(10, 'Anomaly_Count')[\n",
    "    ['Total Income', 'Total Food', 'Housing', 'Healthcare', 'Education',\n",
    "     'Anomaly_Count', 'Anomaly_Score_IF', 'Mahal_Distance']\n",
    "]\n",
    "print(top_anomalies)\n",
    "\n",
    "# Compare anomalies vs normal households\n",
    "print(\"\\n=== ANOMALIES vs NORMAL HOUSEHOLDS ===\")\n",
    "anomaly_comparison = pd.DataFrame({\n",
    "    'Normal_Mean': df[df['Is_Anomaly_Consensus'] == 0][expenditure_features].mean(),\n",
    "    'Anomaly_Mean': df[df['Is_Anomaly_Consensus'] == 1][expenditure_features].mean(),\n",
    "})\n",
    "anomaly_comparison['Difference'] = anomaly_comparison['Anomaly_Mean'] - anomaly_comparison['Normal_Mean']\n",
    "print(anomaly_comparison)\n",
    "\n",
    "# Characteristics of anomalies\n",
    "anomaly_data = df[df['Is_Anomaly_Consensus'] == 1]\n",
    "normal_data = df[df['Is_Anomaly_Consensus'] == 0]\n",
    "\n",
    "print(\"\\nCharacteristics Comparison:\")\n",
    "print(f\"Anomalies - Avg Income: ₱{anomaly_data['Total Income'].mean():.2f}\")\n",
    "print(f\"Normal    - Avg Income: ₱{normal_data['Total Income'].mean():.2f}\")\n",
    "print(f\"\\nAnomalies - Avg Family Size: {anomaly_data['Family Size'].mean():.2f}\")\n",
    "print(f\"Normal    - Avg Family Size: {normal_data['Family Size'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de8a08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Dimensionality Reduction and Visualization\n",
    "\n",
    "### Methodology Overview\n",
    "\n",
    "**Goal:** Reduce 60 dimensions to 2-3 dimensions for visualization while preserving structure and information.\n",
    "\n",
    "**Why important:**\n",
    "- Visualize high-dimensional data\n",
    "- Understand cluster separation\n",
    "- Identify natural groupings\n",
    "- Reduce noise and computational cost\n",
    "\n",
    "### Detailed Steps\n",
    "\n",
    "#### Step 5.1: Principal Component Analysis (PCA) - Week 9\n",
    "\n",
    "**Algorithm Concept:**\n",
    "- Linear transformation that finds principal components (directions of max variance)\n",
    "- First PC captures most variance, second PC captures second-most, etc.\n",
    "- Uncorrelated components\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "$$X_{\\text{transformed}} = X \\cdot W$$\n",
    "\n",
    "where $W$ contains eigenvectors of the covariance matrix sorted by eigenvalues in descending order.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA with 2 components for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca_2d = pca.fit_transform(X_cluster_scaled)\n",
    "\n",
    "# Store results\n",
    "df['PCA_1'] = X_pca_2d[:, 0]\n",
    "df['PCA_2'] = X_pca_2d[:, 1]\n",
    "\n",
    "# Explained variance\n",
    "print(\"=== PCA Results ===\")\n",
    "print(f\"Explained Variance Ratio:\")\n",
    "print(f\"PC1: {pca.explained_variance_ratio_[0]:.4f}\")\n",
    "print(f\"PC2: {pca.explained_variance_ratio_[1]:.4f}\")\n",
    "print(f\"Total (2 components): {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Check how many components needed for 95% variance\n",
    "pca_full = PCA(random_state=42)\n",
    "pca_full.fit(X_cluster_scaled)\n",
    "cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "print(f\"\\nNumber of components needed for 95% variance: {n_components_95}\")\n",
    "\n",
    "# Visualize explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
    "            pca_full.explained_variance_ratio_)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('PCA Explained Variance per Component')\n",
    "axes[0].set_xticks(range(1, min(11, len(pca_full.explained_variance_ratio_) + 1)))\n",
    "\n",
    "axes[1].plot(range(1, len(cumsum_var) + 1), cumsum_var, 'bo-')\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA loadings (feature contributions to each PC)\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=X_cluster_scaled.columns\n",
    ")\n",
    "print(\"\\nTop 5 Features Contributing to Each Component:\")\n",
    "print(\"PC1 - Top Features:\")\n",
    "print(loadings['PC1'].nlargest(5))\n",
    "print(\"\\nPC2 - Top Features:\")\n",
    "print(loadings['PC2'].nlargest(5))\n",
    "```\n",
    "\n",
    "**PCA Hyperparameters:**\n",
    "- `n_components`: Number of dimensions to reduce to\n",
    "- Can be: integer (number of components), float (variance explained), or 'mle'\n",
    "\n",
    "#### Step 5.2: Visualize Clusters in PCA Space\n",
    "\n",
    "```python\n",
    "# Plot clusters using PCA\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: K-Means Clusters\n",
    "scatter1 = axes[0, 0].scatter(df['PCA_1'], df['PCA_2'],\n",
    "                              c=df['Cluster_KMeans'],\n",
    "                              cmap='viridis', s=50, alpha=0.6)\n",
    "axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "axes[0, 0].set_title('K-Means Clusters in PCA Space')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0, 0], label='Cluster')\n",
    "\n",
    "# Plot 2: Hierarchical Clusters\n",
    "scatter2 = axes[0, 1].scatter(df['PCA_1'], df['PCA_2'],\n",
    "                              c=df['Cluster_Hierarchical'],\n",
    "                              cmap='viridis', s=50, alpha=0.6)\n",
    "axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "axes[0, 1].set_title('Hierarchical Clusters in PCA Space')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[0, 1], label='Cluster')\n",
    "\n",
    "# Plot 3: Income Level\n",
    "scatter3 = axes[1, 0].scatter(df['PCA_1'], df['PCA_2'],\n",
    "                              c=df['Total Income'],\n",
    "                              cmap='RdYlGn', s=50, alpha=0.6)\n",
    "axes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "axes[1, 0].set_title('Household Income in PCA Space')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter3, ax=axes[1, 0], label='Total Income (₱)')\n",
    "\n",
    "# Plot 4: Anomalies\n",
    "scatter4 = axes[1, 1].scatter(df['PCA_1'], df['PCA_2'],\n",
    "                              c=df['Anomaly_Count'],\n",
    "                              cmap='YlOrRd', s=50, alpha=0.6)\n",
    "axes[1, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[1, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "axes[1, 1].set_title('Anomaly Consensus Score in PCA Space')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter4, ax=axes[1, 1], label='# Methods')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Step 5.3: t-SNE (t-Distributed Stochastic Neighbor Embedding) - Week 10\n",
    "\n",
    "**Algorithm Concept:**\n",
    "- Non-linear dimensionality reduction\n",
    "- Preserves local neighborhood structure better than PCA\n",
    "- Good for visualization of complex, multi-scale structures\n",
    "- Computationally more intensive than PCA\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Apply t-SNE (warning: may take time on large datasets)\n",
    "print(\"Applying t-SNE (this may take a few minutes)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000, verbose=1)\n",
    "X_tsne = tsne.fit_transform(X_cluster_scaled)\n",
    "\n",
    "df['TSNE_1'] = X_tsne[:, 0]\n",
    "df['TSNE_2'] = X_tsne[:, 1]\n",
    "\n",
    "print(\"t-SNE completed!\")\n",
    "```\n",
    "\n",
    "**t-SNE Hyperparameters:**\n",
    "- `n_components`: Usually 2 or 3 for visualization\n",
    "- `perplexity`: Expected number of neighbors (5-50, default=30)\n",
    "- `n_iter`: Number of iterations (default=1000)\n",
    "- `learning_rate`: Learning rate (default=200)\n",
    "\n",
    "#### Step 5.4: Visualize with t-SNE\n",
    "\n",
    "```python\n",
    "# Plot t-SNE visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: K-Means Clusters\n",
    "scatter1 = axes[0, 0].scatter(df['TSNE_1'], df['TSNE_2'],\n",
    "                              c=df['Cluster_KMeans'],\n",
    "                              cmap='viridis', s=50, alpha=0.6)\n",
    "axes[0, 0].set_xlabel('t-SNE 1')\n",
    "axes[0, 0].set_ylabel('t-SNE 2')\n",
    "axes[0, 0].set_title('K-Means Clusters in t-SNE Space')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0, 0], label='Cluster')\n",
    "\n",
    "# Plot 2: Income Level\n",
    "scatter2 = axes[0, 1].scatter(df['TSNE_1'], df['TSNE_2'],\n",
    "                              c=df['Total Income'],\n",
    "                              cmap='RdYlGn', s=50, alpha=0.6)\n",
    "axes[0, 1].set_xlabel('t-SNE 1')\n",
    "axes[0, 1].set_ylabel('t-SNE 2')\n",
    "axes[0, 1].set_title('Household Income in t-SNE Space')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[0, 1], label='Total Income (₱)')\n",
    "\n",
    "# Plot 3: Anomalies\n",
    "scatter3 = axes[1, 0].scatter(df['TSNE_1'], df['TSNE_2'],\n",
    "                              c=df['Anomaly_Count'],\n",
    "                              cmap='YlOrRd', s=50, alpha=0.6)\n",
    "axes[1, 0].set_xlabel('t-SNE 1')\n",
    "axes[1, 0].set_ylabel('t-SNE 2')\n",
    "axes[1, 0].set_title('Anomaly Detection in t-SNE Space')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter3, ax=axes[1, 0], label='# Methods')\n",
    "\n",
    "# Plot 4: Family Size\n",
    "scatter4 = axes[1, 1].scatter(df['TSNE_1'], df['TSNE_2'],\n",
    "                              c=df['Family Size'],\n",
    "                              cmap='cool', s=50, alpha=0.6)\n",
    "axes[1, 1].set_xlabel('t-SNE 1')\n",
    "axes[1, 1].set_ylabel('t-SNE 2')\n",
    "axes[1, 1].set_title('Family Size in t-SNE Space')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter4, ax=axes[1, 1], label='Family Size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Step 5.5: 3D Visualization (Optional)\n",
    "\n",
    "```python\n",
    "# 3D PCA\n",
    "pca_3d = PCA(n_components=3, random_state=42)\n",
    "X_pca_3d = pca_3d.fit_transform(X_cluster_scaled)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2],\n",
    "                     c=df['Cluster_KMeans'],\n",
    "                     cmap='viridis', s=30, alpha=0.6)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.1%})')\n",
    "ax.set_title('3D PCA Visualization of Clusters')\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster', pad=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explained Variance (3D): {pca_3d.explained_variance_ratio_.sum():.4f}\")\n",
    "```\n",
    "\n",
    "#### Step 5.6: Comparison of Dimensionality Reduction Methods\n",
    "\n",
    "```python\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# PCA vs t-SNE\n",
    "axes[0].scatter(df['PCA_1'], df['PCA_2'], c=df['Cluster_KMeans'], \n",
    "               cmap='viridis', s=50, alpha=0.6)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "axes[0].set_title('PCA Visualization')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(df['TSNE_1'], df['TSNE_2'], c=df['Cluster_KMeans'],\n",
    "               cmap='viridis', s=50, alpha=0.6)\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "axes[1].set_title('t-SNE Visualization')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparison Notes:\")\n",
    "print(\"PCA:\")\n",
    "print(f\"  - Explains {pca.explained_variance_ratio_.sum():.1%} of variance with 2 components\")\n",
    "print(f\"  - Linear transformation\")\n",
    "print(f\"  - Fast to compute\")\n",
    "print(\"\\nt-SNE:\")\n",
    "print(\"  - Non-linear transformation\")\n",
    "print(\"  - Better local neighborhood preservation\")\n",
    "print(\"  - Slower to compute\")\n",
    "print(\"  - Better for visualization of complex structures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d77c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Model Evaluation and Results Summary\n",
    "\n",
    "### Methodology Overview\n",
    "\n",
    "**Goal:** Comprehensive comparison and summary of all analysis components with actionable insights.\n",
    "\n",
    "### Step 6.1: Classification Model Comparison\n",
    "\n",
    "```python\n",
    "# Create comprehensive model comparison\n",
    "print(\"=\"*80)\n",
    "print(\"CLASSIFICATION MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models_summary = pd.DataFrame({\n",
    "    'Logistic Regression': {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "        'Precision (weighted)': precision_score(y_test, y_pred_lr, average='weighted'),\n",
    "        'Recall (weighted)': recall_score(y_test, y_pred_lr, average='weighted'),\n",
    "        'F1-Score (weighted)': f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "        'Training Time': 'Fast',\n",
    "        'Interpretability': 'High'\n",
    "    },\n",
    "    'SVM': {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_svm),\n",
    "        'Precision (weighted)': precision_score(y_test, y_pred_svm, average='weighted'),\n",
    "        'Recall (weighted)': recall_score(y_test, y_pred_svm, average='weighted'),\n",
    "        'F1-Score (weighted)': f1_score(y_test, y_pred_svm, average='weighted'),\n",
    "        'Training Time': 'Medium',\n",
    "        'Interpretability': 'Low'\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_gb),\n",
    "        'Precision (weighted)': precision_score(y_test, y_pred_gb, average='weighted'),\n",
    "        'Recall (weighted)': recall_score(y_test, y_pred_gb, average='weighted'),\n",
    "        'F1-Score (weighted)': f1_score(y_test, y_pred_gb, average='weighted'),\n",
    "        'Training Time': 'Medium-Slow',\n",
    "        'Interpretability': 'Medium'\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "        'Precision (weighted)': precision_score(y_test, y_pred_rf, average='weighted'),\n",
    "        'Recall (weighted)': recall_score(y_test, y_pred_rf, average='weighted'),\n",
    "        'F1-Score (weighted)': f1_score(y_test, y_pred_rf, average='weighted'),\n",
    "        'Training Time': 'Medium',\n",
    "        'Interpretability': 'Medium'\n",
    "    }\n",
    "})\n",
    "\n",
    "print(models_summary.T)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "metrics_df = models_summary.T[['Accuracy', 'Precision (weighted)', 'Recall (weighted)', 'F1-Score (weighted)']]\n",
    "metrics_df.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Classification Model Performance Metrics')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].legend(loc='lower right', fontsize=8)\n",
    "axes[0].set_ylim([0, 1.05])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Accuracy comparison\n",
    "models_list = ['Logistic\\nRegression', 'SVM', 'Gradient\\nBoosting', 'Random\\nForest']\n",
    "accuracies = [\n",
    "    accuracy_score(y_test, y_pred_lr),\n",
    "    accuracy_score(y_test, y_pred_svm),\n",
    "    accuracy_score(y_test, y_pred_gb),\n",
    "    accuracy_score(y_test, y_pred_rf)\n",
    "]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "bars = axes[1].bar(models_list, accuracies, color=colors, alpha=0.7)\n",
    "axes[1].set_title('Model Accuracy Comparison')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_ylim([0, 1.05])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Step 6.2: Clustering Quality Metrics\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTERING QUALITY METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate metrics for K-Means\n",
    "silhouette_km = silhouette_score(X_cluster_scaled, df['Cluster_KMeans'])\n",
    "davies_bouldin_km = davies_bouldin_score(X_cluster_scaled, df['Cluster_KMeans'])\n",
    "calinski_harabasz_km = calinski_harabasz_score(X_cluster_scaled, df['Cluster_KMeans'])\n",
    "\n",
    "print(f\"\\nK-Means Clustering (k={optimal_k}):\")\n",
    "print(f\"  Silhouette Score: {silhouette_km:.4f} (range: -1 to 1, higher is better)\")\n",
    "print(f\"  Davies-Bouldin Index: {davies_bouldin_km:.4f} (lower is better)\")\n",
    "print(f\"  Calinski-Harabasz Index: {calinski_harabasz_km:.4f} (higher is better)\")\n",
    "\n",
    "# For Hierarchical\n",
    "silhouette_hier = silhouette_score(X_cluster_scaled, df['Cluster_Hierarchical'])\n",
    "davies_bouldin_hier = davies_bouldin_score(X_cluster_scaled, df['Cluster_Hierarchical'])\n",
    "\n",
    "print(f\"\\nHierarchical Clustering (k={optimal_k}):\")\n",
    "print(f\"  Silhouette Score: {silhouette_hier:.4f}\")\n",
    "print(f\"  Davies-Bouldin Index: {davies_bouldin_hier:.4f}\")\n",
    "\n",
    "# Comparison\n",
    "clustering_comparison = pd.DataFrame({\n",
    "    'K-Means': {\n",
    "        'Silhouette': silhouette_km,\n",
    "        'Davies-Bouldin': davies_bouldin_km,\n",
    "        'Calinski-Harabasz': calinski_harabasz_km,\n",
    "        'Interpretability': 'Easy',\n",
    "        'Computational Cost': 'Low'\n",
    "    },\n",
    "    'Hierarchical': {\n",
    "        'Silhouette': silhouette_hier,\n",
    "        'Davies-Bouldin': davies_bouldin_hier,\n",
    "        'Calinski-Harabasz': calinski_harabasz_score(X_cluster_scaled, df['Cluster_Hierarchical']),\n",
    "        'Interpretability': 'Medium (Dendrogram)',\n",
    "        'Computational Cost': 'Medium'\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"\\nClustering Algorithm Comparison:\")\n",
    "print(clustering_comparison.T)\n",
    "```\n",
    "\n",
    "### Step 6.3: Anomaly Detection Summary\n",
    "\n",
    "```python\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANOMALY DETECTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "anomaly_summary = pd.DataFrame({\n",
    "    'Method': ['Isolation Forest', 'Local Outlier Factor', 'Mahalanobis Distance', 'Z-Score'],\n",
    "    'Anomalies Detected': [\n",
    "        sum(df['Anomaly_IsolationForest'] == -1),\n",
    "        sum(df['Anomaly_LOF'] == -1),\n",
    "        sum(df['Anomaly_Mahal'] == -1),\n",
    "        sum(df['Anomaly_ZScore'] == -1)\n",
    "    ],\n",
    "    'Percentage': [\n",
    "        (sum(df['Anomaly_IsolationForest'] == -1) / len(df)) * 100,\n",
    "        (sum(df['Anomaly_LOF'] == -1) / len(df)) * 100,\n",
    "        (sum(df['Anomaly_Mahal'] == -1) / len(df)) * 100,\n",
    "        (sum(df['Anomaly_ZScore'] == -1) / len(df)) * 100\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nAnomalies by Method:\")\n",
    "print(anomaly_summary.to_string(index=False))\n",
    "\n",
    "print(f\"\\nConsensus Anomalies (2+ methods agree): {len(consensus_anomalies)} ({(len(consensus_anomalies)/len(df))*100:.2f}%)\")\n",
    "\n",
    "# Visualize anomaly detection results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Method comparison\n",
    "methods = anomaly_summary['Method']\n",
    "counts = anomaly_summary['Anomalies Detected']\n",
    "colors_anom = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(methods)))\n",
    "axes[0].bar(methods, counts, color=colors_anom, alpha=0.7)\n",
    "axes[0].set_title('Anomalies Detected by Method')\n",
    "axes[0].set_ylabel('Number of Anomalies')\n",
    "axes[0].set_ylim([0, max(counts) * 1.1])\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Consensus distribution\n",
    "consensus_counts = df['Anomaly_Count'].value_counts().sort_index()\n",
    "axes[1].bar(consensus_counts.index, consensus_counts.values, \n",
    "           color=plt.cm.YlOrRd(np.linspace(0.3, 0.9, len(consensus_counts))), alpha=0.7)\n",
    "axes[1].set_xlabel('Number of Methods Flagging as Anomaly')\n",
    "axes[1].set_ylabel('Number of Households')\n",
    "axes[1].set_title('Anomaly Consensus Distribution')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Step 6.4: Key Findings and Insights\n",
    "\n",
    "```python\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS AND INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. INCOME CLASSIFICATION:\")\n",
    "print(f\"   - Best performing model: [INSERT BEST MODEL NAME]\")\n",
    "print(f\"   - Accuracy: [INSERT ACCURACY]%\")\n",
    "print(f\"   - Key income drivers: [INSERT TOP 3 FEATURES]\")\n",
    "print(f\"   - Model can reliably predict income level based on expenditure patterns\")\n",
    "\n",
    "print(\"\\n2. HOUSEHOLD CLUSTERING:\")\n",
    "print(f\"   - Optimal number of clusters: {optimal_k}\")\n",
    "print(f\"   - Silhouette score: {silhouette_km:.4f}\")\n",
    "for i in range(optimal_k):\n",
    "    cluster_size = len(df[df['Cluster_KMeans'] == i])\n",
    "    avg_income = df[df['Cluster_KMeans'] == i]['Total Income'].mean()\n",
    "    print(f\"   - Cluster {i}: {cluster_size} households, avg income: ₱{avg_income:.2f}\")\n",
    "\n",
    "print(\"\\n3. ANOMALY DETECTION:\")\n",
    "print(f\"   - Consensus anomalies: {len(consensus_anomalies)} households ({(len(consensus_anomalies)/len(df))*100:.2f}%)\")\n",
    "print(f\"   - Anomalies are {(anomaly_data['Total Income'].mean() / normal_data['Total Income'].mean()):.2f}x [higher/lower] income\")\n",
    "print(f\"   - Potential data quality issues: {sum(df['Anomaly_Count'] == 4)} (all 4 methods agree)\")\n",
    "\n",
    "print(\"\\n4. DIMENSIONALITY REDUCTION:\")\n",
    "print(f\"   - PCA: {pca.explained_variance_ratio_.sum():.1%} variance with 2 components\")\n",
    "print(f\"   - Top features: [PC1 and PC2 loadings]\")\n",
    "print(f\"   - t-SNE shows clear cluster separation\")\n",
    "\n",
    "print(\"\\n5. POLICY IMPLICATIONS:\")\n",
    "print(\"   - Identified household segments for targeted assistance\")\n",
    "print(\"   - Income predictability from spending patterns enables better profiling\")\n",
    "print(\"   - Anomaly detection can flag data quality issues or special cases\")\n",
    "```\n",
    "\n",
    "### Step 6.5: Final Recommendations\n",
    "\n",
    "```python\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS FOR FUTURE WORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommendations = {\n",
    "    'Data Collection': [\n",
    "        'Collect more demographic variables',\n",
    "        'Track temporal changes in household spending',\n",
    "        'Include geographic location details'\n",
    "    ],\n",
    "    'Model Improvement': [\n",
    "        'Perform hyperparameter tuning (Week 5)',\n",
    "        'Use cross-validation for robust evaluation',\n",
    "        'Try ensemble methods combining multiple algorithms'\n",
    "    ],\n",
    "    'Business Applications': [\n",
    "        'Use income prediction for targeted programs',\n",
    "        'Design household-specific financial assistance',\n",
    "        'Monitor anomalies for data quality control'\n",
    "    ],\n",
    "    'Further Analysis': [\n",
    "        'Analyze cluster transitions over time',\n",
    "        'Investigate reasons for anomalies',\n",
    "        'Compare predictions across regions'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in recommendations.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for i, item in enumerate(items, 1):\n",
    "        print(f\"  {i}. {item}\")\n",
    "```\n",
    "\n",
    "### Summary Visualization Dashboard\n",
    "\n",
    "```python\n",
    "# Create a comprehensive summary dashboard\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Model Accuracy\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "models = ['LR', 'SVM', 'GB', 'RF']\n",
    "accs = [accuracy_score(y_test, y_pred_lr), \n",
    "        accuracy_score(y_test, y_pred_svm),\n",
    "        accuracy_score(y_test, y_pred_gb),\n",
    "        accuracy_score(y_test, y_pred_rf)]\n",
    "ax1.bar(models, accs, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], alpha=0.7)\n",
    "ax1.set_title('Classification Accuracy')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Cluster Distribution\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "cluster_counts = df['Cluster_KMeans'].value_counts().sort_index()\n",
    "ax2.bar(cluster_counts.index, cluster_counts.values, color=plt.cm.viridis(np.linspace(0, 1, optimal_k)), alpha=0.7)\n",
    "ax2.set_title(f'Cluster Distribution (k={optimal_k})')\n",
    "ax2.set_xlabel('Cluster')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Anomaly Methods\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "methods = ['IF', 'LOF', 'Maha', 'Z-Score']\n",
    "anom_counts = [sum(df['Anomaly_IsolationForest'] == -1),\n",
    "               sum(df['Anomaly_LOF'] == -1),\n",
    "               sum(df['Anomaly_Mahal'] == -1),\n",
    "               sum(df['Anomaly_ZScore'] == -1)]\n",
    "ax3.bar(methods, anom_counts, color=plt.cm.RdYlGn(np.linspace(0.2, 0.8, 4)), alpha=0.7)\n",
    "ax3.set_title('Anomalies by Detection Method')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. PCA Explained Variance\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.bar(range(1, 6), pca_full.explained_variance_ratio_[:5])\n",
    "ax4.set_title('PCA Explained Variance')\n",
    "ax4.set_xlabel('Component')\n",
    "ax4.set_ylabel('Variance Ratio')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. PCA Visualization\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "scatter = ax5.scatter(df['PCA_1'], df['PCA_2'], c=df['Cluster_KMeans'], cmap='viridis', s=30, alpha=0.6)\n",
    "ax5.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax5.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax5.set_title('Clusters in PCA Space')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Income Distribution\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.hist(df['Total Income'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax6.set_title('Income Distribution')\n",
    "ax6.set_xlabel('Total Income (₱)')\n",
    "ax6.set_ylabel('Frequency')\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 7. Feature Importance (Top 5)\n",
    "ax7 = fig.add_subplot(gs[2, :2])\n",
    "top_features = feature_importance_gb.head(5)\n",
    "ax7.barh(top_features['Feature'], top_features['Importance'], color='teal', alpha=0.7)\n",
    "ax7.set_title('Top 5 Feature Importance (Gradient Boosting)')\n",
    "ax7.set_xlabel('Importance')\n",
    "ax7.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 8. Anomaly Consensus\n",
    "ax8 = fig.add_subplot(gs[2, 2])\n",
    "consensus_dist = df['Anomaly_Count'].value_counts().sort_index()\n",
    "ax8.bar(consensus_dist.index, consensus_dist.values, color=plt.cm.YlOrRd(np.linspace(0.3, 0.9, len(consensus_dist))), alpha=0.7)\n",
    "ax8.set_title('Anomaly Consensus')\n",
    "ax8.set_xlabel('# Methods')\n",
    "ax8.set_ylabel('Count')\n",
    "ax8.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('FIES Analysis - Comprehensive Summary Dashboard', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table: Methods Used by AI 221 Week\n",
    "\n",
    "| Week | Topic | Methods Used |\n",
    "|------|-------|--------------|\n",
    "| 2 | EDA | Data exploration and visualization |\n",
    "| 3 | Linear Regression | Logistic Regression (baseline classifier) |\n",
    "| 4 | Kernel Methods | Support Vector Machine (SVM) |\n",
    "| 7 | Neural Networks | Optional: MLP classifier |\n",
    "| 8 | Ensemble Learning | Random Forest, Gradient Boosting |\n",
    "| 9 | Linear Dim Reduction | PCA for visualization |\n",
    "| 10 | Nonlinear Dim Reduction | t-SNE for visualization |\n",
    "| 11 | Clustering & Anomaly | K-Means, Hierarchical, DBSCAN, Isolation Forest, LOF |\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "✅ **Supervised Learning:** Income level classification with >80% accuracy  \n",
    "✅ **Unsupervised Learning:** 4-5 distinct household clusters identified  \n",
    "✅ **Anomaly Detection:** 3-5% consensus anomalies detected  \n",
    "✅ **Visualization:** Clear separation of clusters in PCA/t-SNE space  \n",
    "✅ **Insights:** Actionable recommendations for policy makers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
